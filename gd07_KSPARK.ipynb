{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d7656a",
   "metadata": {},
   "source": [
    "# GD14 multiface detection  \n",
    "## Objective  \n",
    "1. multiface detection을 위한 widerface 데이터셋의 전처리가 적절히 진행되었다.\n",
    "- tfrecord 생성, augmentation, prior box 생성 등의 과정이 정상적으로 진행되었다.\n",
    "2. SSD 모델이 안정적으로 학습되어 multiface detection이 가능해졌다.\n",
    "- inference를 통해 정확한 위치의 face bounding box를 detect한 결과이미지가 제출되었다.\n",
    "3. 이미지 속 다수의 얼굴에 스티커가 적용되었다.\n",
    "- 이미지 속 다수의 얼굴의 적절한 위치에 스티커가 적용된 결과이미지가 제출되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd28efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, time\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'widerface')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
    "TRAIN_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'train_mask.tfrecord')\n",
    "VALID_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'val_mask.tfrecord')\n",
    "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
    "\n",
    "DATASET_LEN = 12880\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_WIDTH = 320\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_LABELS = ['background', 'face']\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b81816",
   "metadata": {},
   "source": [
    "## 데이터셋 전처리  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc673615",
   "metadata": {},
   "source": [
    "프로젝트에서 사용하는 데이터셋은 WIDER FACE 데이터셋입니다  \n",
    "http://shuoyang1213.me/WIDERFACE/index.html  \n",
    "보다 넓은 공간에 있는 다수의 사람이 등장하는 이미지 데이터셋으로, pose, expression, makeup, illumination이 각각 다른 데이터들도 많이 포함되어 있습니다  \n",
    "빠른 inference보다는 먼 거리에 흩어져 있는 여러 사람의 얼굴을 빠르게 detect하는 모델을 만들기 위해서 사용하기 적합합니다  \n",
    "데이터셋 생성에 앞서, 프로젝트에 필요한 형식대로 전처리를 수행합니다  \n",
    "\n",
    "### 데이터셋 \n",
    "데이터셋 내의 X 좌표, Y 좌표, 너비, 높이를 런타임 중 활용하기 위한 함수를 선언합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5784d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852714a",
   "metadata": {},
   "source": [
    "이미지별 bounding box 정보를 wider_face_train_bbx_gt.txt에서 파싱해서 리스트로 추출합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7163742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_widerface(file):\n",
    "    infos = []\n",
    "    with open(file) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            n_object = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(n_object):\n",
    "                box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = parse_box(box)\n",
    "                if (w == 0) or (h == 0):\n",
    "                    continue\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if n_object == 0:\n",
    "                box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = parse_box(box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            infos.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1b2ec",
   "metadata": {},
   "source": [
    "추출한 정보를 실제 이미지 정보와 결합합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec7a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c5e68",
   "metadata": {},
   "source": [
    "bounding box [x, y, w, h] >> [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe4569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fc141",
   "metadata": {},
   "source": [
    "### TFRecord 생성  \n",
    "대용량 데이터셋의 처리속도 향상을 위해서, 전처리 작업을 통해 TFRecord 데이터셋으로 변환하도록 합니다  \n",
    "TensorFlow만의 학습 데이터 저장 포맷인 TFRecord는 이진(binary) 레코드의 시퀀스를 저장합니다  \n",
    "학습 속도의 개선을 기대할 수 있습니다  \n",
    "#### 구성  \n",
    "여러 개의 tf.train.Example로 이루어져 있고, 한 개의 tf.train.Example은 여러 개의 tf.train.Feature로 이루어져 있습니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0774d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(image_string, image_infos):\n",
    "    for info in image_infos:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432d9a2",
   "metadata": {},
   "source": [
    "TF 레코드 writer를 통해, 위 make_example 메소드에서 만든 example을 serialize합니다  \n",
    "결과물로 TFRecord 파일이 생성됩니다  \n",
    "https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4688ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12880/12880 [01:05<00:00, 196.86it/s]\n",
      "100%|██████████| 3226/3226 [00:13<00:00, 235.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'val']:\n",
    "    if split == 'train':\n",
    "        output_file = TRAIN_TFRECORD_PATH \n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train'\n",
    "    else:\n",
    "        output_file = VALID_TFRECORD_PATH\n",
    "        anno_txt = 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_val'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(DATA_PATH, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(DATA_PATH, file_path, 'images', info[0])\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "                writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99741e35",
   "metadata": {},
   "source": [
    "## 모델 구현  \n",
    "이번 프로젝트에서 사용하는 모델은 SSD (single shot detector) 입니다  \n",
    "Yolo와 같은 one stage detector입니다   \n",
    "https://manalelaidouni.github.io/Single%20shot%20object%20detection.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac83222",
   "metadata": {},
   "source": [
    "### Default box 생성  \n",
    "Default box란, object가 존재할 만한 다양한 크기의 box의 좌표 및 클래스 정보를 일정 개수만큼 미리 고정해 둔 것입니다  \n",
    "즉 anchor box의 기능을 수행하는 것입니다  \n",
    "SSD에서는 여러 층의 feature map에서 다양한 크기의 box를 만들어 냅니다  \n",
    "훈련 과정에서, ground truth에 해당하는 bounding box와의 IoU를 계산하여 일정 크기(0.5) 이상 겹치는 default box를 선택하는 방식으로 사용도며, RCNN 계열의 sliding window 방식보다 훨씬 속도가 빠르면서도 그와 유사한 정도의 정확도를 얻을 수 있었습니다  \n",
    "  \n",
    "이번 프로젝트에서 사용할 Default box 정보를 전역 변수로 만들어두겠습니다  \n",
    "SSD에서는 기준이 되는 feature map을 먼저 생성한 후에 default box를 생성합니다. \n",
    "프로젝트에서는 아래와 같이 4가지 유형, 8x8, 16x16,,, 의 feature map을 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34d52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_MIN_SIZES = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]\n",
    "BOX_STEPS = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83998063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 40], [16, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "min_sizes = BOX_MIN_SIZES\n",
    "steps= BOX_STEPS\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps\n",
    "]\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95399a9",
   "metadata": {},
   "source": [
    " feature map별로 순회를 하면서 default box 를 생성해봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf328f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = []\n",
    "for k, f in enumerate(feature_maps):\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            s_kx = min_size / image_sizes[1]\n",
    "            s_ky = min_size / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            boxes += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ab7c5",
   "metadata": {},
   "source": [
    "생성된 boxes에는 default box 정보가 구분없이 나열되어 있으므로 4개씩 재배열시켜주어야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ccfc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4700, 4)\n",
      "[[0.0125    0.015625  0.03125   0.0390625]\n",
      " [0.0125    0.015625  0.05      0.0625   ]\n",
      " [0.0125    0.015625  0.075     0.09375  ]\n",
      " ...\n",
      " [0.9       0.875     0.4       0.5      ]\n",
      " [0.9       0.875     0.6       0.75     ]\n",
      " [0.9       0.875     0.8       1.       ]]\n"
     ]
    }
   ],
   "source": [
    "pretty_boxes = np.asarray(boxes).reshape([-1, 4])\n",
    "print(pretty_boxes.shape)\n",
    "print(pretty_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b91b5e",
   "metadata": {},
   "source": [
    "모두 4700개의 default box가 만들어졌습니다  \n",
    "feature_maps와 min_sizes로부터 40x32x3 + 20x16x2 + 10x8x2 + 5x4x3 개가 생성되었다는 걸 확인할 수 있었습니다  \n",
    "  \n",
    "재사용성을 위하여 함수화 시켜줍니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c1942ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_box():\n",
    "    image_sizes = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    min_sizes = BOX_MIN_SIZES\n",
    "    steps= BOX_STEPS\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps\n",
    "    ]\n",
    "    boxes = []\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                s_kx = min_size / image_sizes[1]\n",
    "                s_ky = min_size / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                boxes += [cx, cy, s_kx, s_ky]\n",
    "    boxes = np.asarray(boxes).reshape([-1, 4])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab50980",
   "metadata": {},
   "source": [
    "### SSD model 빌드   \n",
    "SSD 모델 내부에서 사용하는 레이어들을 다음과 같이 생성합니다  \n",
    "- Convolution Block  \n",
    "- Depthwide Convolution Block  \n",
    "- Branch Block - skip connection 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fce3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1)):\n",
    "    block_id = (tf.keras.backend.get_uid())\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd4c25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, filters, strides=(1, 1)):\n",
    "    block_id = tf.keras.backend.get_uid()\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b08192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(inputs, filters):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(inputs)\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65479bb5",
   "metadata": {},
   "source": [
    "여러 블록을 쌓아 만들어진 모델은, 중간중간 Branch부분에 헤드(head)라 불리는 convolution 레이어가 필요합니다  \n",
    "하나의 헤드는 2개의 convolution layer를 사용하며, 하나는 confidence 예측을,다른 하나는 location 예측을 수행합니다  \n",
    "Branch마다 헤드가 연결되어 있기 때문에, 모델의 중간 레이어에서도 예측을 위한 정보를 가져올 수 있습니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1358e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe5f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(inputs, num_class, num_cell):\n",
    "    conf = _create_head_block(inputs, num_cell * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs, num_cell * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43f9f8",
   "metadata": {},
   "source": [
    "선언한 블록을 사용하여 SSD 모델을 준비합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47561e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SsdModel():\n",
    "    base_channel = 16\n",
    "    num_cells = [3, 2, 2, 3]\n",
    "    num_class = len(IMAGE_LABELS)\n",
    "    \n",
    "    x = inputs = tf.keras.layers.Input(shape=[IMAGE_HEIGHT, IMAGE_WIDTH, 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    for layer, num_cell in zip(extra_layers, num_cells):\n",
    "        conf, loc = _compute_heads(layer, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name='ssd_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6d9462f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of model layers:  101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 256, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_15 (ZeroPadding2D)     (None, 258, 322, 3)  0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 128, 160, 16) 432         conv_pad_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_15 (BatchNormalization) (None, 128, 160, 16) 64          conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_15 (ReLU)             (None, 128, 160, 16) 0           conv_bn_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 128, 160, 32) 4608        conv_relu_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_16 (BatchNormalization) (None, 128, 160, 32) 128         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_16 (ReLU)             (None, 128, 160, 32) 0           conv_bn_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_17 (ZeroPadding2D)     (None, 130, 162, 32) 0           conv_relu_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 64, 80, 32)   9216        conv_pad_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_17 (BatchNormalization) (None, 64, 80, 32)   128         conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_17 (ReLU)             (None, 64, 80, 32)   0           conv_bn_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 64, 80, 32)   9216        conv_relu_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_18 (BatchNormalization) (None, 64, 80, 32)   128         conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_18 (ReLU)             (None, 64, 80, 32)   0           conv_bn_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_19 (ZeroPadding2D)     (None, 66, 82, 32)   0           conv_relu_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 32, 40, 64)   18432       conv_pad_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_19 (BatchNormalization) (None, 32, 40, 64)   256         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_19 (ReLU)             (None, 32, 40, 64)   0           conv_bn_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 32, 40, 64)   36864       conv_relu_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_20 (BatchNormalization) (None, 32, 40, 64)   256         conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_20 (ReLU)             (None, 32, 40, 64)   0           conv_bn_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 32, 40, 64)   36864       conv_relu_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_21 (BatchNormalization) (None, 32, 40, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_21 (ReLU)             (None, 32, 40, 64)   0           conv_bn_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 32, 40, 64)   36864       conv_relu_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_22 (BatchNormalization) (None, 32, 40, 64)   256         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_22 (ReLU)             (None, 32, 40, 64)   0           conv_bn_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_23 (ZeroPadding2D)     (None, 34, 42, 64)   0           conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 16, 20, 128)  73728       conv_pad_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_23 (BatchNormalization) (None, 16, 20, 128)  512         conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_23 (ReLU)             (None, 16, 20, 128)  0           conv_bn_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_24 (Conv2D)                (None, 16, 20, 128)  147456      conv_relu_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_24 (BatchNormalization) (None, 16, 20, 128)  512         conv_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_24 (ReLU)             (None, 16, 20, 128)  0           conv_bn_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 16, 20, 128)  147456      conv_relu_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_25 (BatchNormalization) (None, 16, 20, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_25 (ReLU)             (None, 16, 20, 128)  0           conv_bn_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_26 (ZeroPadding2D)     (None, 18, 22, 128)  0           conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_28 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 40, 16)   9232        conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 20, 16)   18448       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 40, 16)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 16, 20, 16)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 8, 10, 16)    0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 4, 5, 16)     0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 40, 16)   2320        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 40, 32)   18464       conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 20, 16)   2320        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 20, 32)   36896       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 40, 48)   0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 20, 48)   0           conv2d_16[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 10, 48)    0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 5, 48)     0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 40, 48)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 16, 20, 48)   0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 8, 10, 48)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 4, 5, 48)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 40, 12)   5196        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 20, 8)    3464        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 40, 6)    2598        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 20, 4)    1732        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3840, 4)      0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 640, 4)       0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 160, 4)       0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 60, 4)        0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3840, 2)      0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 640, 2)       0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 160, 2)       0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 60, 2)        0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, 4700, 4)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, 4700, 2)      0           reshape[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 4700, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SsdModel()\n",
    "print(\"the number of model layers: \", len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213a15a",
   "metadata": {},
   "source": [
    "총 101층의 SSD 모델이 완성되었습니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4d22f",
   "metadata": {},
   "source": [
    "## 데이터셋 준비  \n",
    "앞서 제작한 모델 구조에 맞게 데이터셋을 준비합니다  \n",
    "데이터 어그멘테이션을 수행합니다  \n",
    "IoU를 활용하는 jaccard 유사도를 구하는 함수를 생성해줍니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d755b",
   "metadata": {},
   "source": [
    "### Augmentation  \n",
    "모델의 성능 향상을 위해, train data를 augmentation 합니다  \n",
    "tf.data.TFRecordDataset.map() 내에서 crop, resize, flip, distort, pad_to_square 를 사용하여 이미지를 왜곡시켜 augmentation을 수행합니다  \n",
    "주의할점은, 이미지가 변경되면서 Box의 위치나 크기가 함께 변경된다는 것입니다  \n",
    "  \n",
    "특히, 이미지를 crop했을 때에는, box도 함께 잘릴 수가 있어서, 유념하고 함께 신경써서 영역을 잘라냅니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f8663df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f43d7",
   "metadata": {},
   "source": [
    "resize나 flip 연산 또한 박스에 영향을 끼칩니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ac17392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels):\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "429ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0b975",
   "metadata": {},
   "source": [
    "pad_to_square는 이미지가 정사각형이 되도록 이미지 오른쪽이나 아래 방향으로 평균 색상 영역을 추가해주는 작업입니다  \n",
    "좌표계의 기준이 되는 이미지 왼쪽이나 위쪽으로는 변화가 없기 때문에 box 정보는 변하지 않습니다  \n",
    "이미지의 생각값만 바꾸어주는 distort 또한, 수행 시에 box 정보는 변하지 않습니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0092eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4da48499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc110b8f",
   "metadata": {},
   "source": [
    "### Default box 적용하기  \n",
    "SSD 에서는 default box 정보를 데이터셋에 반영하여 사용합니다  \n",
    "default box와 bounding box의 IoU, 다르게 표현하여 자카드 유사도를 측정하여 사용합니다  \n",
    "\n",
    "#### Jaccard similarity   \n",
    "두 집합의 교집합을, 두 집합의 합집합으로 나눕니다  \n",
    "0과 1 사이의 값을 가지며, 1에 비슷할수록 두 집합의 교집합이 합집합과 비슷한 영역을 차지하게 됩니다  \n",
    "\n",
    "J(X,Y) = (ANB) / (AUB) = (ANB) / A + B - (ANB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9f3d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(box_a, box_b):\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc8644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(box_a, box_b):\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f90f5",
   "metadata": {},
   "source": [
    "TFRecord 데이터셋의 라벨을 다음과 같이 가공하도록 합니다  \n",
    "- jaccard 메소드를 이용해 label의 ground truth bbox와 가장 overlap 비율이 높은 matched box를 구한다  \n",
    "- _encode_bbox 메소드를 통해 bbox의 scale을 동일하게 보정한다  \n",
    "- 전체 default box에 대해 일정 threshold 이상 overlap되는 ground truth bounding box 존재 여부(positive/negative)를 concat하여 \n",
    "- 새로운 label로 업데이트한다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b202645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bbox(matched, boxes, variances=[0.1, 0.2]):\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - boxes[:, :2]\n",
    "    g_cxcy /= (variances[0] * boxes[:, 2:])\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / boxes[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "    g_wh = tf.where(tf.math.is_inf(g_wh), 0.0, g_wh)\n",
    "    return tf.concat([g_cxcy, g_wh], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2795fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, boxes):\n",
    "    match_threshold = 0.45\n",
    "    boxes = tf.cast(boxes, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "   \n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, boxes)\n",
    "    best_box_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_box_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_box_idx, 1),\n",
    "        tf.ones_like(best_box_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_box_idx, 1),\n",
    "        tf.range(tf.size(best_box_idx), dtype=tf.int32))\n",
    "    # Scale Ground-Truth Boxes   \n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)\n",
    "    loc_t = _encode_bbox(matches_bbox, boxes)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_threshold), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146e2bc",
   "metadata": {},
   "source": [
    "### 데이터셋 로드하기  \n",
    "위에서 구현한 두가지 메소드를 tfrecord 데이터셋에 적용, SSD 학습을 위한 데이터셋을 로드하는 함수를 구현합니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b32ab",
   "metadata": {},
   "source": [
    "#### augmemtation과 label을 encoding 하여 기존의 dataset을 변환하는 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23dc1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(train, boxes):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if train:\n",
    "            img, labels = _crop(img, labels)\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        img, labels = _resize(img, labels)\n",
    "\n",
    "        if train:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        if train:\n",
    "            img = _distort(img)\n",
    "        labels = encode_tf(labels, boxes)        \n",
    "        img = img/255.0\n",
    "        return img, labels\n",
    "    return transform_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea570961",
   "metadata": {},
   "source": [
    "#### TFRecord 에 _transform_data를 적용하는 함수 클로저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a6bd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(train, boxes):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(train, boxes)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e0ac5",
   "metadata": {},
   "source": [
    "#### tf.data.TFRecordDataset.map()에 _parse_tfrecord을 적용하는 실제 데이터셋 변환 메인 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c0f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, train=True, boxes=None, buffer_size=1024):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if train:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = raw_dataset.map(_parse_tfrecord(train, boxes), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669b8f5",
   "metadata": {},
   "source": [
    "#### load_tfrecord_dataset을 통해 train, validation 데이터셋을 생성하는 최종 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46f7d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(boxes, train=True, buffer_size=1024):\n",
    "    if train:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=TRAIN_TFRECORD_PATH,\n",
    "            train=train,\n",
    "            boxes=boxes,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=VALID_TFRECORD_PATH,\n",
    "            train=train,\n",
    "            boxes=boxes,\n",
    "            buffer_size=buffer_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301298aa",
   "metadata": {},
   "source": [
    "## 모델 학습  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d9f43",
   "metadata": {},
   "source": [
    "# POC랩 마치고 꼭 완성해보이겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435b7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e0702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8bb596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601472bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225efbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bb3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
