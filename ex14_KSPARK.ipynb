{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f625f236",
   "metadata": {},
   "source": [
    "# EX14 Transformer Chatbot  \n",
    "## Objective  \n",
    "1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.  \n",
    "- 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.  \n",
    "2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.  \n",
    "- 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.  \n",
    "3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.  \n",
    "- 한국어 입력문장에 맥락에 맞는 한국어로 답변을 리턴하였다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18c29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b1f6",
   "metadata": {},
   "source": [
    "### Step 1.  데이터 수집하기\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다  \n",
    "https://github.com/songys/Chatbot_data   \n",
    "일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855ae70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "original_data = pd.read_csv(file_path)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85bf35bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb34f10",
   "metadata": {},
   "source": [
    "총 11823개의 한국어 챗봇 데이터를 불러왔습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc33555",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기  \n",
    "지난 요약 프로젝트에서 사용한 방법입니다  \n",
    "- 중복 제거  \n",
    "- 결측치 제거  \n",
    "- 텍스트 정규화  \n",
    "- 특수문자 제거  \n",
    "- 다시한번 결측치 제거  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd16405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결측치확인\n",
    "original_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713dd036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#중복제거\n",
    "original_data = original_data.drop_duplicates()\n",
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057e52a",
   "metadata": {},
   "source": [
    "결측치와 중복은 없었습니다  \n",
    "정규화를 시켜줍니다  \n",
    "- 문장에서 단어와 구두점 사이에 공백을 추가  \n",
    "- 알파벳(한글)과 ! ? , . 이 4개의 구두점을 제외하고 다른 특수문자는 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56966a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "\n",
    "    # (한글, a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r\"^[ㄱ-ㅎ|가-힣|a-z|A-Z|0-9|]+$\", \" \", sentence)\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05215259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sd카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sd카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sns 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sns 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sns 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sns보면 나만 빼고 다 행복해보여</td>\n",
       "      <td>자랑하는 자리니까요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>가끔 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>가끔은 혼자인게 좋다</td>\n",
       "      <td>혼자를 즐기세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>가난한 자의 설움</td>\n",
       "      <td>돈은 다시 들어올 거예요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>가만 있어도 땀난다</td>\n",
       "      <td>땀을 식혀주세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>가상화폐 쫄딱 망함</td>\n",
       "      <td>어서 잊고 새출발 하세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>가스불 켜고 나갔어</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>가스비 너무 많이 나왔다 .</td>\n",
       "      <td>다음 달에는 더 절약해봐요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>가스비 비싼데 감기 걸리겠어</td>\n",
       "      <td>따뜻하게 사세요 !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>가스비 장난 아님</td>\n",
       "      <td>다음 달에는 더 절약해봐요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>가장 확실한 건 뭘까 ?</td>\n",
       "      <td>가장 확실한 시간은 오늘이에요 . 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>가족 여행 가기로 했어</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>가족 여행 고고</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>가족 여행 어디로 가지 ?</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>가족 있어 ?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님 , 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>가족관계 알려 줘</td>\n",
       "      <td>저를 만들어 준 사람을 부모님 , 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>가족끼리 여행간다 .</td>\n",
       "      <td>더 가까워질 기회가 되겠네요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>가족들 보고 싶어</td>\n",
       "      <td>저도요 .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q  \\\n",
       "0                   12시 땡 !   \n",
       "1               1지망 학교 떨어졌어   \n",
       "2              3박4일 놀러가고 싶다   \n",
       "3           3박4일 정도 놀러가고 싶다   \n",
       "4                   ppl 심하네   \n",
       "5                 sd카드 망가졌어   \n",
       "6                   sd카드 안돼   \n",
       "7            sns 맞팔 왜 안하지ㅠㅠ   \n",
       "8   sns 시간낭비인 거 아는데 매일 하는 중   \n",
       "9         sns 시간낭비인데 자꾸 보게됨   \n",
       "10      sns보면 나만 빼고 다 행복해보여   \n",
       "11                   가끔 궁금해   \n",
       "12              가끔 뭐하는지 궁금해   \n",
       "13              가끔은 혼자인게 좋다   \n",
       "14                가난한 자의 설움   \n",
       "15               가만 있어도 땀난다   \n",
       "16               가상화폐 쫄딱 망함   \n",
       "17               가스불 켜고 나갔어   \n",
       "18           가스불 켜놓고 나온거 같아   \n",
       "19          가스비 너무 많이 나왔다 .   \n",
       "20          가스비 비싼데 감기 걸리겠어   \n",
       "21                가스비 장난 아님   \n",
       "22            가장 확실한 건 뭘까 ?   \n",
       "23             가족 여행 가기로 했어   \n",
       "24                 가족 여행 고고   \n",
       "25           가족 여행 어디로 가지 ?   \n",
       "26                  가족 있어 ?   \n",
       "27                가족관계 알려 줘   \n",
       "28              가족끼리 여행간다 .   \n",
       "29                가족들 보고 싶어   \n",
       "\n",
       "                                                    A  label  \n",
       "0                                         하루가 또 가네요 .      0  \n",
       "1                                          위로해 드립니다 .      0  \n",
       "2                                        여행은 언제나 좋죠 .      0  \n",
       "3                                        여행은 언제나 좋죠 .      0  \n",
       "4                                         눈살이 찌푸려지죠 .      0  \n",
       "5                                 다시 새로 사는 게 마음 편해요 .      0  \n",
       "6                                 다시 새로 사는 게 마음 편해요 .      0  \n",
       "7                                   잘 모르고 있을 수도 있어요 .      0  \n",
       "8                                      시간을 정하고 해보세요 .      0  \n",
       "9                                      시간을 정하고 해보세요 .      0  \n",
       "10                                       자랑하는 자리니까요 .      0  \n",
       "11                                     그 사람도 그럴 거예요 .      0  \n",
       "12                                     그 사람도 그럴 거예요 .      0  \n",
       "13                                         혼자를 즐기세요 .      0  \n",
       "14                                    돈은 다시 들어올 거예요 .      0  \n",
       "15                                         땀을 식혀주세요 .      0  \n",
       "16                                    어서 잊고 새출발 하세요 .      0  \n",
       "17                               빨리 집에 돌아가서 끄고 나오세요 .      0  \n",
       "18                               빨리 집에 돌아가서 끄고 나오세요 .      0  \n",
       "19                                   다음 달에는 더 절약해봐요 .      0  \n",
       "20                                         따뜻하게 사세요 !      0  \n",
       "21                                   다음 달에는 더 절약해봐요 .      0  \n",
       "22  가장 확실한 시간은 오늘이에요 . 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마...      0  \n",
       "23                         온 가족이 모두 마음에 드는 곳으로 가보세요 .      0  \n",
       "24                         온 가족이 모두 마음에 드는 곳으로 가보세요 .      0  \n",
       "25                         온 가족이 모두 마음에 드는 곳으로 가보세요 .      0  \n",
       "26     저를 만들어 준 사람을 부모님 , 저랑 이야기해 주는 사람을 친구로 생각하고 있어요      0  \n",
       "27     저를 만들어 준 사람을 부모님 , 저랑 이야기해 주는 사람을 친구로 생각하고 있어요      0  \n",
       "28                                  더 가까워질 기회가 되겠네요 .      0  \n",
       "29                                              저도요 .      0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = original_data.copy()\n",
    "data.Q = data.Q.apply(preprocess_sentence)\n",
    "data.A = data.A.apply(preprocess_sentence)\n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3768242",
   "metadata": {},
   "source": [
    "요구사항이 잘 맞춰진 것 같습니다  \n",
    "다시한번 결측치를 확인한 후, 있다면 제거해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b927bb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결측치확인\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fb0e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', ..., '흑기사 해주는 짝남 .',\n",
       "       '힘든 연애 좋은 연애라는게 무슨 차이일까 ?', '힘들어서 결혼할까봐'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이후 단계를 준비해줍니다\n",
    "questions = data.Q.values.copy()\n",
    "answers = data.A.values.copy()\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce027e",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기  \n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다  \n",
    "하지만 여기서는 형태소 분석기가 아닌 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해봅니다  \n",
    "- TensorFlow Datasets SubwordTextEncoder 를 토크나이저로 사용하기\n",
    "- 단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고,  각 토큰을 고유한 정수로 인코딩하기\n",
    "- 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가하기\n",
    "- 최대 길이 MAX_LENGTH를 넘는 문장들은 필터링 / MAX_LENGTH보다 길이가 짧은 문장들은 범위에 맞도록 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20331ab3",
   "metadata": {},
   "source": [
    "단어장을 생성합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8d1dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df0f91",
   "metadata": {},
   "source": [
    "target_vocab_size = 2^13이 적절할까요?  \n",
    "퍼포먼스를 봐서, EX12에 했던 단어 갯수를 세는 함수를 사용해서 탐색해봐도 괜찮을 거 같습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14bcf86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8269"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더의 문장 생성 과정에서 사용할 '시작 토큰'과 '종료 토큰'\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "# 단어장의 크기\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57c9bf",
   "metadata": {},
   "source": [
    "8269의 크기를 가진 단어장이 생성되었습니다  \n",
    "정수 인코딩을 진행합니다  \n",
    "동시에 MAX_LENGTH를 넘는 샘플은 제거 이후, 패딩을 진행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9bbe568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5e4ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 전의 질문 샘플 개수: 11823\n",
      "필터링 전의 답변 샘플 개수: 11823\n",
      "단어장의 크기 : 8269\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('필터링 전의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 전의 답변 샘플 개수: {}'.format(len(answers)))\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6ce6a",
   "metadata": {},
   "source": [
    "샘플갯수의 변화는 없었습니다  \n",
    "패딩이 많을수록 추론 효과가 적다고 들었어서, 이후 추론결과가 좋지 않다면 개선할 수 있는 부분으로 기록해두겠습니다 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ac2eb",
   "metadata": {},
   "source": [
    " 교사 강요(Teacher Forcing)를 사용하는 입력 파이프라인을 만듭니다  \n",
    "answers[:, :-1]를 디코더의 입력값, answers[:, 1:]를 디코더의 레이블로 사용합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbf3f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "becfd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbace8a4",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e4ff7",
   "metadata": {},
   "source": [
    "트랜스포머 함수를 정의합니다  \n",
    "진행에 필요한 함수들 또한 함께 정의합니다  \n",
    "  \n",
    "먼저 패딩 마스킹 함수를 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba31625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5163e4",
   "metadata": {},
   "source": [
    "룩 어헤드 마스킹 함수도 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7a7ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a90e4d",
   "metadata": {},
   "source": [
    "스케일드 닷 프로덕트 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b9113b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae4fad",
   "metadata": {},
   "source": [
    "멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "48f6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ac15c",
   "metadata": {},
   "source": [
    "포지셔널 인코딩 함수입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "166074e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    " \n",
    "    def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dae6c",
   "metadata": {},
   "source": [
    "인코더를 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6559a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "    epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    #두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    #완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "395d2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa0b2f",
   "metadata": {},
   "source": [
    "디코더를 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "89237410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9103e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "      outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed16e1",
   "metadata": {},
   "source": [
    "대망의 트랜스포머 함수입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "af54e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d8fc6",
   "metadata": {},
   "source": [
    "손실함수를 정의합니다  \n",
    "cross entropy를 쓰는군요  \n",
    "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2dfe1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520954f5",
   "metadata": {},
   "source": [
    "Model의 Learning rate를 스케쥴링 할 수 있는 커스텀 스케쥴러를 정의합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "87513889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98da61",
   "metadata": {},
   "source": [
    "드디어 모델을 정의합니다  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "62817979",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a4a1d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3171072     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3698432     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8269)   2125133     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,994,637\n",
      "Trainable params: 8,994,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcbdfc",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed21f2",
   "metadata": {},
   "source": [
    "컴파일~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "88bb94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cfacfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 25s 54ms/step - loss: 1.4576 - accuracy: 0.0188\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.1826 - accuracy: 0.0489\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 1.0064 - accuracy: 0.0508\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.9288 - accuracy: 0.0544\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.8705 - accuracy: 0.0577\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.8109 - accuracy: 0.0620\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.7451 - accuracy: 0.0678\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.6732 - accuracy: 0.0756\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.5950 - accuracy: 0.0842\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.5137 - accuracy: 0.0937\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.4323 - accuracy: 0.1037\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.3530 - accuracy: 0.1142\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.2794 - accuracy: 0.1247\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.2145 - accuracy: 0.1346\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1606 - accuracy: 0.1440\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1181 - accuracy: 0.1519\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0880 - accuracy: 0.1575\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0696 - accuracy: 0.1606\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0590 - accuracy: 0.1626\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0538 - accuracy: 0.1635\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0499 - accuracy: 0.1639\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0477 - accuracy: 0.1645\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0446 - accuracy: 0.1651\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0401 - accuracy: 0.1661\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0355 - accuracy: 0.1674\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0334 - accuracy: 0.1677\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0299 - accuracy: 0.1686\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0281 - accuracy: 0.1690\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0260 - accuracy: 0.1696\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0241 - accuracy: 0.1702\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0232 - accuracy: 0.1703\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0215 - accuracy: 0.1706\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0210 - accuracy: 0.1708\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0198 - accuracy: 0.1711\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0195 - accuracy: 0.1711\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0181 - accuracy: 0.1715\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0179 - accuracy: 0.1715\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0166 - accuracy: 0.1718\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0168 - accuracy: 0.1717\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0162 - accuracy: 0.1719\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0157 - accuracy: 0.1720\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0155 - accuracy: 0.1721\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0149 - accuracy: 0.1722\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0147 - accuracy: 0.1722\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0140 - accuracy: 0.1723\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0139 - accuracy: 0.1725\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0135 - accuracy: 0.1725\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0130 - accuracy: 0.1726\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0131 - accuracy: 0.1726\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0128 - accuracy: 0.1727\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "69ee3471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (10, 4))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2329cac",
   "metadata": {},
   "source": [
    "학습은 30 에폭 이후 accuracy 와 loss가 수렴했습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92edd78",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기  \n",
    "모델이 잘 학습되었는지 테스트해봅니다  \n",
    "추론 함수를 정의합니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5baf88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "  \n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c95bc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "240fa8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 멍멍\n",
      "출력 : 많이 힘든가봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'많이 힘든가봅니다 .'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"멍멍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1de22d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 자고싶다\n",
      "출력 : 많이 힘든가봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'많이 힘든가봅니다 .'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"자고싶다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2f539411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 굿모닝\n",
      "출력 : 많이 힘든가봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'많이 힘든가봅니다 .'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"굿모닝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a879a3",
   "metadata": {},
   "source": [
    "에러인가 싶었습니다 ㄷㄷㄷ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8eb48266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 한잔할래?\n",
      "출력 : 분위기 있네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'분위기 있네요 .'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"한잔할래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "02d31fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨가 좋아요\n",
      "출력 : 집밖에 나가기가 힘들것 같아요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'집밖에 나가기가 힘들것 같아요 .'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"오늘 날씨가 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eda42e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 사랑해요\n",
      "출력 : 많이 힘든가봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'많이 힘든가봅니다 .'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"사랑해요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f73566",
   "metadata": {},
   "source": [
    "왜 이러는걸까요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f6761f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나한테 왜 이래?\n",
      "출력 : 존재 자체로 큰 힘이 되어주고 있나봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'존재 자체로 큰 힘이 되어주고 있나봅니다 .'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"나한테 왜 이래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f2c8643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 보고싶다\n",
      "출력 : 많이 힘든가봅니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'많이 힘든가봅니다 .'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"보고싶다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7113192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 많이 힘들다\n",
      "출력 : 그 과정을 통해 성장할 수 있을 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그 과정을 통해 성장할 수 있을 거예요 .'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"많이 힘들다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3d2aee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 아직 잘 모르겠어요\n",
      "출력 : 사랑과 결혼을 동일시하지 않아도 돼요 . 시간을 가지고 생각해보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'사랑과 결혼을 동일시하지 않아도 돼요 . 시간을 가지고 생각해보세요 .'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"아직 잘 모르겠어요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9af99",
   "metadata": {},
   "source": [
    "가끔은 엉뚱하지만 대체로 듣고싶은 말, 혹은 문맥에 맞지만 놀라운 말을 본 것 같습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d323fe3",
   "metadata": {},
   "source": [
    "# 회고 \n",
    "결과물은 굉장히 재미있었습니다  \n",
    "### 인상깊은 점  \n",
    "데이터셋이 적기 때문에 매우 엉성한 결과를 기대했는데, 의외로 어느정도 컨텍스트를 확인할 수 있었습니다  \n",
    "### 아쉬운점  \n",
    "공부를 하면서 작성을 했지만, 결국 함수들을 모두 새롭게 짜보지는 못하고, 가져다 쓰게 되었습니다 ..  \n",
    "NLP쪽도 잘하고 싶었는데, 결국 마지막 노드가 될 이번 EX도 다양한 실험을 해보며 익숙해지지 못해서 더 아쉬움이 큰 것 같습니다  \n",
    "### 기대되는 점\n",
    "해당 데이터셋은 아래 페이지에 명시되어 있듯이, '이별' 상황에서 듣는 따뜻한 문장들에서 데이터를 모아 만들었습니다  \n",
    "https://github.com/songys/Chatbot_data  \n",
    "실제로 구현된 챗봇과 대화하며, 왠지 이녀석은 따뜻하군~ 이라고 생각했습니다  \n",
    "결국 데이터셋을 한정하면, '성격' '감성' 을 가진(듯한) 챗봇을 구현할 수 있을거라 기대됩니다  \n",
    "  \n",
    "참고 : https://wikidocs.net/89786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa0a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
